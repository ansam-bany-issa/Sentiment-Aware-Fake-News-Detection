{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5m2FTVt3rXHS"
      },
      "outputs": [],
      "source": [
        "pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ewUR8M2sff1"
      },
      "outputs": [],
      "source": [
        "pip install keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4wIsyQoRsh3C"
      },
      "outputs": [],
      "source": [
        "pip install textblob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2h9JFExshuK"
      },
      "outputs": [],
      "source": [
        "pip install scikit-learn "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92IcCIxAqv2t"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from textblob import TextBlob\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, Conv1D, MaxPooling1D, Dropout\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('sentiment_data.csv')\n",
        "\n",
        "# Preprocessing\n",
        "df.drop_duplicates(inplace=True)\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Add TextBlob sentiment analysis scores as features\n",
        "df['polarity'] = df['Tweet'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
        "df['subjectivity'] = df['Tweet'].apply(lambda x: TextBlob(x).sentiment.subjectivity)\n",
        "\n",
        "# Split data into train and test sets\n",
        "train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Tokenize and pad sequences\n",
        "tokenizer = Tokenizer(num_words=10000, lower=True)\n",
        "tokenizer.fit_on_texts(train_data['Tweet'])\n",
        "x_train = pad_sequences(tokenizer.texts_to_sequences(train_data['Tweet']), maxlen=100)\n",
        "x_test = pad_sequences(tokenizer.texts_to_sequences(test_data['Tweet']), maxlen=100)\n",
        "\n",
        "# Define labels for classification\n",
        "y_train = train_data['label'].values\n",
        "y_test = test_data['label'].values\n",
        "\n",
        "# Define CNN model\n",
        "model = Sequential()\n",
        "model.add(Embedding(10000, 128, input_length=100))\n",
        "model.add(Conv1D(64, 5, activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=4))\n",
        "model.add(LSTM(128))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10, batch_size=64)\n",
        "\n",
        "# Evaluate the model on test data\n",
        "loss, accuracy = model.evaluate(x_test, y_test, verbose=False)\n",
        "print(f'Test accuracy: {accuracy*100}')\n",
        "\n",
        "# Classify new tweets using the trained model and TextBlob sentiment analysis scores\n",
        "new_tweets = ['This is a Fake news tweet!', 'This is a Real news tweet', 'This is a Neutral news tweet.']\n",
        "new_tweet_features = pd.DataFrame({'text': new_tweets})\n",
        "new_tweet_features['polarity'] = new_tweet_features['text'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
        "new_tweet_features['subjectivity'] = new_tweet_features['text'].apply(lambda x: TextBlob(x).sentiment.subjectivity)\n",
        "new_tweet_sequences = pad_sequences(tokenizer.texts_to_sequences(new_tweet_features['text']), maxlen=100)\n",
        "new_tweet_probabilities = model.predict(new_tweet_sequences)\n",
        "\n",
        "# Print the predicted labels for the new tweets\n",
        "for i in range(len(new_tweets)):\n",
        "    if new_tweet_probabilities[i][0] < 0:\n",
        "        print(f'Tweet \"{new_tweets[i]}\" is classified as FAKE')\n",
        "    elif new_tweet_probabilities[i][0] > 0:\n",
        "        print(f'Tweet \"{new_tweets[i]}\" is classified as REAL')\n",
        "    else:    \n",
        "        print(f'Tweet \"{new_tweets[i]}\" is classified as Neutral')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPCMry3AsbNk"
      },
      "outputs": [],
      "source": [
        "# Load the old tweets dataset\n",
        "old_tweet_features = pd.read_csv('after_pre.csv')\n",
        "\n",
        "# Load the tokenizer and model\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(old_tweet_features['Tweet'])\n",
        "\n",
        "# Generate random sample of tweets from the dataset for testing\n",
        "test_sample = old_tweet_features.sample(n=25)\n",
        "\n",
        "# Classify the sample tweets using the trained model and TextBlob sentiment analysis scores\n",
        "test_sample_sequences = pad_sequences(tokenizer.texts_to_sequences(test_sample['Tweet']), maxlen=100)\n",
        "test_sample_probabilities = model.predict(test_sample_sequences)\n",
        "\n",
        "# Regenerate the predicted probabilities with the correct shape\n",
        "test_sample_probabilities = model.predict(test_sample_sequences)\n",
        "\n",
        "# Create a new column for predicted labels in the test sample DataFrame\n",
        "test_sample['label'] = ['REAL' if p > 0 else ('FAKE' if p < 0 else 'NEUTRAL') for p in test_sample_probabilities]\n",
        "\n",
        "\n",
        "# Print the predicted labels for the sample tweets\n",
        "for i in range(len(test_sample)):\n",
        "    if test_sample_probabilities[i][0] < 0:\n",
        "        print(f'Tweet \"{test_sample.iloc[i][\"Tweet\"]}\" is classified as FAKE')\n",
        "    elif test_sample_probabilities[i][0] > 0:\n",
        "        print(f'Tweet \"{test_sample.iloc[i][\"Tweet\"]}\" is classified as REAL')\n",
        "    else:\n",
        "        print(f'Tweet \"{test_sample.iloc[i][\"Tweet\"]}\" is classified as Neutral')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
